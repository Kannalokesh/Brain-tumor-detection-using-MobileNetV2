{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "568c3b68-016f-400f-84f6-9a74302226f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd19b558-e0a7-4140-86dc-cb336a6b5c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2400 images belonging to 2 classes.\n",
      "Found 600 images belonging to 2 classes.\n",
      "\n",
      "Class indices: {'no': 0, 'yes': 1}\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "image_size = (224, 224)\n",
    "channels=3\n",
    "batch_size = 32\n",
    "epochs = 25\n",
    "data_path = 'C:/Users/junjo/Documents/python/projects/brain_tumor_detection/dataset'\n",
    "\n",
    "# Data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_path,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    data_path,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Verify class indices\n",
    "print(\"\\nClass indices:\", train_generator.class_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8176cf0-902b-413b-bb6a-ad27f5adb9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 2s/step - accuracy: 0.5652 - loss: 0.9185 - val_accuracy: 0.8183 - val_loss: 0.4295\n",
      "Epoch 2/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 2s/step - accuracy: 0.7448 - loss: 0.5585 - val_accuracy: 0.8650 - val_loss: 0.3437\n",
      "Epoch 3/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - accuracy: 0.7977 - loss: 0.4399 - val_accuracy: 0.8667 - val_loss: 0.3005\n",
      "Epoch 4/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 2s/step - accuracy: 0.8022 - loss: 0.4475 - val_accuracy: 0.8917 - val_loss: 0.2597\n",
      "Epoch 5/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 2s/step - accuracy: 0.8379 - loss: 0.3704 - val_accuracy: 0.9083 - val_loss: 0.2455\n",
      "Epoch 6/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 2s/step - accuracy: 0.8587 - loss: 0.3581 - val_accuracy: 0.9200 - val_loss: 0.2143\n",
      "Epoch 7/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 2s/step - accuracy: 0.8568 - loss: 0.3652 - val_accuracy: 0.9283 - val_loss: 0.1936\n",
      "Epoch 8/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 2s/step - accuracy: 0.8551 - loss: 0.3255 - val_accuracy: 0.9317 - val_loss: 0.2097\n",
      "Epoch 9/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 2s/step - accuracy: 0.8700 - loss: 0.3035 - val_accuracy: 0.9250 - val_loss: 0.2053\n",
      "Epoch 10/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 2s/step - accuracy: 0.8766 - loss: 0.2877 - val_accuracy: 0.9167 - val_loss: 0.2155\n",
      "Epoch 11/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - accuracy: 0.8739 - loss: 0.2940 - val_accuracy: 0.9317 - val_loss: 0.1876\n",
      "Epoch 12/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - accuracy: 0.8775 - loss: 0.2793 - val_accuracy: 0.9250 - val_loss: 0.1990\n",
      "Epoch 13/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - accuracy: 0.8656 - loss: 0.3205 - val_accuracy: 0.9217 - val_loss: 0.2169\n",
      "Epoch 14/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 1s/step - accuracy: 0.8794 - loss: 0.2965 - val_accuracy: 0.9300 - val_loss: 0.1993\n",
      "Epoch 15/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 1s/step - accuracy: 0.8731 - loss: 0.3115 - val_accuracy: 0.9483 - val_loss: 0.1737\n",
      "Epoch 16/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - accuracy: 0.8822 - loss: 0.2965 - val_accuracy: 0.9367 - val_loss: 0.1633\n",
      "Epoch 17/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 1s/step - accuracy: 0.8816 - loss: 0.2744 - val_accuracy: 0.9400 - val_loss: 0.1732\n",
      "Epoch 18/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 1s/step - accuracy: 0.8775 - loss: 0.2785 - val_accuracy: 0.9400 - val_loss: 0.1681\n",
      "Epoch 19/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - accuracy: 0.8776 - loss: 0.3009 - val_accuracy: 0.9500 - val_loss: 0.1593\n",
      "Epoch 20/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 1s/step - accuracy: 0.8799 - loss: 0.2891 - val_accuracy: 0.9417 - val_loss: 0.1623\n",
      "Epoch 21/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 1s/step - accuracy: 0.8819 - loss: 0.2896 - val_accuracy: 0.9417 - val_loss: 0.1819\n",
      "Epoch 22/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 1s/step - accuracy: 0.8779 - loss: 0.2701 - val_accuracy: 0.9467 - val_loss: 0.1620\n",
      "Epoch 23/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 1s/step - accuracy: 0.8933 - loss: 0.2471 - val_accuracy: 0.9483 - val_loss: 0.1668\n",
      "Epoch 24/25\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 1s/step - accuracy: 0.8996 - loss: 0.2442 - val_accuracy: 0.9367 - val_loss: 0.1838\n"
     ]
    }
   ],
   "source": [
    "# Model architecture\n",
    "\n",
    "base_model = MobileNetV2(\n",
    "    input_shape=image_size + (3,),\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    pooling='avg'\n",
    ")\n",
    "\n",
    "# Freeze base model layers initially\n",
    "base_model.trainable = False\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile with lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, monitor='val_loss', restore_best_weights=True),\n",
    "    ModelCheckpoint('best_brain_model.keras', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Training\n",
    "my_model = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "130c6608-2d0f-4634-85ab-450ff8778a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Tumor detected!\n"
     ]
    }
   ],
   "source": [
    "# Prediction function\n",
    "def predict_tumor(img_path):\n",
    "    img = tf.keras.utils.load_img(img_path, target_size=image_size)\n",
    "    x = tf.keras.utils.img_to_array(img)\n",
    "    x = preprocess_input(x)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    \n",
    "    pred = model.predict(x)\n",
    "    return 'No tumor found' if np.argmax(pred) == 0 else 'Tumor detected!'\n",
    "\n",
    "# Example usage\n",
    "test_image = 'C:/Users/junjo/Documents/python/projects/brain_tumor_detection/test_data/pred/pred9.jpg'\n",
    "print(predict_tumor(test_image))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48763e88-4c59-418b-a16d-b45c55350801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
